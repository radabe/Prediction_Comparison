{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbaaa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loguniform(low=0, high=1):\n",
    "    val = np.exp(np.random.uniform(0, 1, None))\n",
    "    scaled_val = (((val - np.exp(0)) * (high - low)) / (np.exp(1) - np.exp(0))) + low\n",
    "    return scaled_val\n",
    "\n",
    "def loguniform_int(low=0, high=1):\n",
    "    val = np.exp(np.random.uniform(0, 1, None))\n",
    "    scaled_val = (((val - np.exp(0)) * (high - low)) / (np.exp(1) - np.exp(0))) + low\n",
    "    return int(scaled_val)\n",
    "\n",
    "def uniform(low=0, high=1):\n",
    "    val = np.random.uniform(low, high, None)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557f2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(data, max_len):\n",
    "    data = data.sort_values('complete_timestamp', ascending=True, kind=\"mergesort\").groupby('unique_id_ros').head(max_len)\n",
    "    grouped = data.sort_values('complete_timestamp', ascending=True, kind=\"mergesort\").groupby('unique_id_ros')\n",
    "\n",
    "    data_dim = data.shape[1]-8\n",
    "    n_cases = data['unique_id'].nunique()\n",
    "\n",
    "    X = np.zeros((n_cases, max_len, data_dim), dtype=np.float32)\n",
    "    y = np.zeros((n_cases, 2), dtype=np.float32)\n",
    "\n",
    "    idx = 0\n",
    "    # each prefix will be a separate instance\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values('complete_timestamp', ascending=True, kind=\"mergesort\")\n",
    "        label = group['Releasetreue'].iloc[0]\n",
    "        group = group.to_numpy()\n",
    "        X[idx] = pad_sequences(group[np.newaxis,:30,8:], maxlen=max_len, dtype=np.float32)\n",
    "        y[idx, label] = 1\n",
    "        idx += 1\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e689e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCHistory(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val_static, X_val_dynamic, y_val):\n",
    "        self.X_val_dynamic = X_val_dynamic\n",
    "        self.X_val_static = X_val_static\n",
    "        self.y_val = y_val\n",
    "        self.aucs = []\n",
    "        self.aucs_pr_0 = []\n",
    "        self.aucs_pr_1 = []\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.aucs = []\n",
    "        self.aucs_pr = []\n",
    "        self.aucs_pr_0 = []\n",
    "        self.aucs_pr_1 = []\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict([self.X_val_static, self.X_val_dynamic])\n",
    "        self.aucs.append(roc_auc_score(self.y_val[:,0], y_pred[:,0]))\n",
    "        precision, recall, _ = precision_recall_curve(self.y_val[:,0],  y_pred[:,0])\n",
    "        self.aucs_pr_0.append(auc(recall, precision))\n",
    "        precision, recall, _ = precision_recall_curve(self.y_val[:,1],  y_pred[:,1])\n",
    "        self.aucs_pr_1.append(auc(recall, precision))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08b7f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynamicSubnetwork(max_len, data_dim_dynamic, n_layers, lstmsize, dropout):\n",
    "    \n",
    "    dynamic_input = Input(shape=(max_len, data_dim_dynamic), name='dynamic_input')\n",
    "    if n_layers == 1:\n",
    "        l2_3 = LSTM(lstmsize, input_shape=(max_len, data_dim_dynamic), implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(dynamic_input)\n",
    "        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "    elif n_layers == 2:\n",
    "        l1 = LSTM(lstmsize, input_shape=(max_len, data_dim_dynamic), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(dynamic_input)\n",
    "        b1 = BatchNormalization(axis=1)(l1)\n",
    "        l2_3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b1)\n",
    "        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "    elif n_layers == 3:\n",
    "        l1 = LSTM(lstmsize, input_shape=(max_len, data_dim_dynamic), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(dynamic_input)\n",
    "        b1 = BatchNormalization(axis=1)(l1)\n",
    "        l2 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(b1)\n",
    "        b2 = BatchNormalization(axis=1)(l2)\n",
    "        l3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b2)\n",
    "        b2_3 = BatchNormalization()(l3)\n",
    "            \n",
    "    dynamic_output=Dropout(dropout,name='Dropout_Layer_Dynamic')(b2_3)\n",
    "\n",
    "    model = Model(inputs=dynamic_input, outputs=dynamic_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def StaticSubnetwork(data_dim_static, n_layers, dropout ):\n",
    "    \n",
    "    static_input = Input(shape=(data_dim_static), name='static_input')\n",
    "    if n_layers == 1:\n",
    "        l2_3= Dense(8, activation='relu', kernel_initializer='glorot_uniform')(static_input)\n",
    "        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "    elif n_layers == 2:\n",
    "        l1 = Dense(16, activation='relu', kernel_initializer='glorot_uniform')(static_input)\n",
    "        b1 = BatchNormalization(axis=1)(l1)\n",
    "        l2_3 = Dense(8, activation='relu', kernel_initializer='glorot_uniform')(b1)\n",
    "        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "    elif n_layers == 3:\n",
    "        l1 = Dense(32, activation='relu', kernel_initializer='glorot_uniform')(static_input)\n",
    "        b1 = BatchNormalization(axis=1)(l1)\n",
    "        l2 = Dense(16, activation='relu', kernel_initializer='glorot_uniform')(b1)\n",
    "        b2 = BatchNormalization(axis=1)(l2)\n",
    "        l3 = Dense(8, activation='relu', kernel_initializer='glorot_uniform')(b2)\n",
    "        b2_3 = BatchNormalization()(l3)\n",
    "            \n",
    "    static_output=Dropout(dropout,name='Dropout_Layer_Static')(b2_3)\n",
    "\n",
    "    model = Model(inputs=static_input, outputs=static_output)\n",
    "    return model\n",
    "\n",
    "def DeepMultimodalModel(max_len, data_dim_dynamic, data_dim_static, n_layers, lstmsize, dropout, activation):\n",
    "    \n",
    "    static_subnet = StaticSubnetwork(data_dim_static, n_layers, dropout )\n",
    "    dynamic_subnet = DynamicSubnetwork(max_len, data_dim_dynamic, n_layers, lstmsize, dropout)\n",
    "    \n",
    "    x = concatenate([static_subnet.output, dynamic_subnet.output]) \n",
    "    \n",
    "    x = Dense(8, activation='relu', name='final_dense_layer_')(x)\n",
    "    \n",
    "    outcome_output = Dense(2, activation=activation, kernel_initializer='glorot_uniform', name='outcome_output')(x)\n",
    "    \n",
    "    model = Model(inputs=[static_subnet.input, dynamic_subnet.input], outputs=outcome_output)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aedc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 5.0 started at 2022-11-17 14:04:39.239731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2806/2806 - 97s - loss: 0.5212 - auc: 0.8191 - val_loss: 0.5411 - val_auc: 0.8242\n",
      "Epoch 2/50\n",
      "2806/2806 - 91s - loss: 0.4517 - auc: 0.8654 - val_loss: 0.5080 - val_auc: 0.8365\n",
      "Epoch 3/50\n",
      "2806/2806 - 91s - loss: 0.4295 - auc: 0.8805 - val_loss: 0.5265 - val_auc: 0.8376\n",
      "Epoch 4/50\n",
      "2806/2806 - 91s - loss: 0.4130 - auc: 0.8909 - val_loss: 0.5078 - val_auc: 0.8445\n",
      "Epoch 5/50\n",
      "2806/2806 - 91s - loss: 0.4036 - auc: 0.8962 - val_loss: 0.5092 - val_auc: 0.8455\n",
      "Epoch 6/50\n",
      "2806/2806 - 91s - loss: 0.3965 - auc: 0.8995 - val_loss: 0.4968 - val_auc: 0.8503\n",
      "Epoch 7/50\n",
      "2806/2806 - 91s - loss: 0.3858 - auc: 0.9063 - val_loss: 0.4914 - val_auc: 0.8520\n",
      "Epoch 8/50\n",
      "2806/2806 - 91s - loss: 0.3813 - auc: 0.9087 - val_loss: 0.5025 - val_auc: 0.8534\n",
      "Epoch 9/50\n",
      "2806/2806 - 91s - loss: 0.3735 - auc: 0.9124 - val_loss: 0.4995 - val_auc: 0.8533\n",
      "Epoch 10/50\n",
      "2806/2806 - 91s - loss: 0.3692 - auc: 0.9150 - val_loss: 0.5020 - val_auc: 0.8573\n",
      "Epoch 11/50\n",
      "2806/2806 - 91s - loss: 0.3666 - auc: 0.9157 - val_loss: 0.4967 - val_auc: 0.8637\n",
      "Epoch 12/50\n",
      "2806/2806 - 91s - loss: 0.3594 - auc: 0.9191 - val_loss: 0.5069 - val_auc: 0.8588\n",
      "Epoch 13/50\n",
      "2806/2806 - 91s - loss: 0.3558 - auc: 0.9208 - val_loss: 0.4964 - val_auc: 0.8589\n",
      "Epoch 14/50\n",
      "2806/2806 - 91s - loss: 0.3507 - auc: 0.9234 - val_loss: 0.5012 - val_auc: 0.8609\n",
      "Epoch 15/50\n",
      "2806/2806 - 91s - loss: 0.3472 - auc: 0.9254 - val_loss: 0.5079 - val_auc: 0.8588\n",
      "Epoch 16/50\n",
      "2806/2806 - 91s - loss: 0.3439 - auc: 0.9265 - val_loss: 0.5288 - val_auc: 0.8616\n",
      "Epoch 17/50\n",
      "2806/2806 - 91s - loss: 0.3398 - auc: 0.9283 - val_loss: 0.5241 - val_auc: 0.8605\n",
      "Epoch 18/50\n",
      "2806/2806 - 91s - loss: 0.3367 - auc: 0.9290 - val_loss: 0.5200 - val_auc: 0.8584\n",
      "Epoch 19/50\n"
     ]
    }
   ],
   "source": [
    "#hypermarameter tuning\n",
    "best_auc=0\n",
    "best_param={}\n",
    "all_auc=[]\n",
    "all_param=[]\n",
    "best_aucpr=0\n",
    "all_aucpr=[]\n",
    "best_parampr={}\n",
    "time=[]\n",
    "confusion=[]\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "nb_epoch = 50\n",
    "start=datetime.now()\n",
    "max_len=30\n",
    "data_dim_static=train_static_ready.shape[1] - 4\n",
    "data_dim_dynamic=train_dynamic_ready.shape[1] - 7\n",
    "n_layers_values = [1, 2, 3]\n",
    "batch_size_values = [8, 16, 32, 64]\n",
    "optimizer_values = [\"rmsprop\", \"nadam\"]\n",
    "\n",
    "for i in range(5,6):\n",
    "     for k in range(1):\n",
    "        \n",
    "        print('run ' + str(i) + '.' + str(k) + ' started at ' + str(datetime.now()))\n",
    "        np.random.seed(i)\n",
    "        train_split = train_static_ready.reindex(np.random.permutation(train_static_ready.index))\n",
    "        val_ids = list(train_split['EC batch'].unique())[-int(val_ratio*len(train_split['EC batch'].unique())):]\n",
    "        val_dynamic_pre = train_dynamic_ready[train_dynamic_ready['EC batch'].isin(val_ids)]\n",
    "        train_dynamic_pre = train_dynamic_ready[~train_dynamic_ready['EC batch'].isin(val_ids)]\n",
    "        val_static_pre = train_static_ready[train_static_ready['EC batch'].isin(val_ids)]\n",
    "        train_static_pre = train_static_ready[~train_static_ready['EC batch'].isin(val_ids)]\n",
    "\n",
    "        del train_split\n",
    "        \n",
    "        X_train_dynamic, y_train_dynamic = generate_3d_data(train_dynamic_pre,max_len)      \n",
    "        X_val_dynamic,y_val_dynamic = generate_3d_data(val_dynamic_pre,max_len)\n",
    "        \n",
    "        X_train_static=train_static_pre.drop(['Adherence','EC batch','id'], axis=1).to_numpy()\n",
    "        X_val_static=val_static_pre.drop(['Adherence','EC batch','id'], axis=1).to_numpy()\n",
    "        \n",
    "        lstmsize = loguniform_int(10, 150)\n",
    "        dropout = uniform(0, 0.3)\n",
    "        n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "        batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "        optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "        learning_rate = loguniform(low=0.000001, high=0.0001)    \n",
    "\n",
    "        if optimizer == \"nadam\":\n",
    "            opt = Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            opt = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "        model= DeepMultimodalModel(max_len, data_dim_dynamic, data_dim_static, n_layers, lstmsize, dropout, activation)\n",
    "        \n",
    "        model.compile(loss={'outcome_output':'binary_crossentropy'}, optimizer=opt, metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "        auc_cb = AUCHistory(X_val_static, X_val_dynamic, y_val_dynamic)\n",
    "        history = model.fit(x=[X_train_static,X_train_dynamic],y=y_train_dynamic, validation_data=([X_val_static, X_val_dynamic], y_val_dynamic),\n",
    "                            verbose=2, callbacks=[auc_cb], batch_size=batch_size, epochs=nb_epoch)\n",
    "\n",
    "        pr_auc=auc_cb.aucs_pr_1[-1]\n",
    "        lr_auc=auc_cb.aucs[-1]\n",
    "\n",
    "\n",
    "\n",
    "        if lr_auc>best_auc:\n",
    "            best_auc=lr_auc\n",
    "            best_param={'run ' +str(i)+'.'+str(k),'lstmsize: ' +str(lstmsize), lstmsize, 'dropout: ' +str(dropout), dropout, 'n_layers: ' +str(n_layers), n_layers,\n",
    "                        'batch_size: ' +str(batch_size), batch_size, 'optimizer:  '+str(optimizer), optimizer,'learning_rate:  '+str(learning_rate),learning_rate}\n",
    "\n",
    "        if pr_auc>best_aucpr:\n",
    "            best_aucpr=pr_auc\n",
    "            best_parampr={'run ' +str(i)+'.'+str(k),'lstmsize: ' +str(lstmsize), lstmsize, 'dropout: ' +str(dropout), dropout, 'n_layers: ' +str(n_layers), n_layers,\n",
    "                        'batch_size: ' +str(batch_size), batch_size, 'optimizer:  '+str(optimizer), optimizer,'learning_rate:  '+str(learning_rate),learning_rate}\n",
    "\n",
    "        all_auc.append(lr_auc)\n",
    "        all_aucpr.append(pr_auc)\n",
    "        all_param.append(['lstmsize= ' +str(lstmsize),'dropout= ' +str(dropout),'n_layers= ' +str(n_layers),'batch_size= ' +str(batch_size),\n",
    "                          'optimizer= ' +str(optimizer),'learning_rate= ' +str(learning_rate)])\n",
    "\n",
    "\n",
    "        print('run ' + str(i) + ' ended at ' + str(datetime.now()))\n",
    "        \n",
    "print('cv ended at ' + str(datetime.now()))\n",
    "\n",
    "end=datetime.now()\n",
    "time.append(end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final testing \n",
    "\n",
    "best_auc=0\n",
    "best_param={}\n",
    "all_auc=[]\n",
    "all_param=[]\n",
    "best_aucpr=0\n",
    "all_aucpr=[]\n",
    "best_parampr={}\n",
    "time=[]\n",
    "confusion=[]\n",
    "\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "thres_pr_list=[]\n",
    "fpr_list=[]\n",
    "tpr_list=[]\n",
    "thres_roc_list=[]\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "nb_epoch = 50\n",
    "start=datetime.now()\n",
    "max_len=30\n",
    "data_dim_static=train_static_ready.shape[1] - 4\n",
    "data_dim_dynamic=train_dynamic_ready.shape[1] - 7\n",
    "\n",
    "X_test_static = test_static_ready.drop(['Adherence','id','EC batch'],axis=1).to_numpy()\n",
    "X_test_dynamic, y_test_dynamic = generate_3d_data(test_dynamic_ready,max_len)\n",
    "\n",
    "\n",
    "for i in range(1,2):\n",
    "     for k in range(20):\n",
    "        \n",
    "        print('run ' + str(i) + '.' + str(k) + ' started at ' + str(datetime.now()))\n",
    "        np.random.seed(i)\n",
    "        train_split = train_static_ready.reindex(np.random.permutation(train_static_ready.index))\n",
    "        val_ids = list(train_split['EC batch'].unique())[-int(val_ratio*len(train_split['EC batch'].unique())):]\n",
    "        val_dynamic_pre = train_dynamic_ready[train_dynamic_ready['EC batch'].isin(val_ids)]\n",
    "        train_dynamic_pre = train_dynamic_ready[~train_dynamic_ready['EC batch'].isin(val_ids)]\n",
    "        val_static_pre = train_static_ready[train_static_ready['EC batch'].isin(val_ids)]\n",
    "        train_static_pre = train_static_ready[~train_static_ready['EC batch'].isin(val_ids)]\n",
    "\n",
    "        del train_split\n",
    "             \n",
    "        X_train_dynamic, y_train_dynamic = generate_3d_data(train_dynamic_pre,max_len)      \n",
    "        X_val_dynamic,y_val_dynamic = generate_3d_data(val_dynamic_pre,max_len)\n",
    "        \n",
    "        X_train_static=train_static_pre.drop(['Adherence','EC batch','id'], axis=1).to_numpy()\n",
    "        X_val_static=val_static_pre.drop(['Adherence','EC batch','id'], axis=1).to_numpy()\n",
    "        del val_dynamic_pre\n",
    "        del train_dynamic_pre, train_static_pre\n",
    "\n",
    "        \n",
    "        lstmsize = loguniform_int(10, 150)\n",
    "        dropout = uniform(0, 0.3)\n",
    "        n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "        batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "        optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "        learning_rate = loguniform(low=0.000001, high=0.0001)    \n",
    "        \n",
    "        np.random.seed(i+k)\n",
    "\n",
    "        if optimizer == \"nadam\":\n",
    "            opt = Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            opt = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "        model= DeepMultimodalModel(max_len, data_dim_dynamic, data_dim_static, n_layers, lstmsize, dropout, activation)\n",
    "        \n",
    "        model.compile(loss={'outcome_output':'binary_crossentropy'}, optimizer=opt, metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "        auc_cb = AUCHistory(X_val_static, X_val_dynamic, y_val_dynamic)\n",
    "        history = model.fit(x=[X_train_static,X_train_dynamic],y=y_train_dynamic, validation_data=([X_val_static, X_val_dynamic], y_val_dynamic),\n",
    "                            verbose=2, callbacks=[auc_cb], batch_size=batch_size, epochs=nb_epoch)\n",
    "        \n",
    "        \n",
    "        predictions=model.predict([X_test_static, X_test_dynamic])\n",
    "        \n",
    "        \n",
    "        # Data to plot precision - recall curve\n",
    "        precision, recall, thres_pr = precision_recall_curve(y_test_dynamic[:,1], predictions[:,1])\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        thres_pr_list.append(thres_pr)\n",
    "        \n",
    "        # Data to plot roc curve\n",
    "        fpr, tpr, thres_roc = roc_curve(y_test_dynamic[:,1],predictions[:,1])\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "        thres_roc_list.append(thres_roc)\n",
    "        \n",
    "        # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "        all_aucpr.append(auc(recall, precision))\n",
    "        \n",
    "        # calculate AUC ROC value\n",
    "        all_auc.append(roc_auc_score(y_test_dynamic[:,1],predictions[:,1]))\n",
    "\n",
    "        print('run ' + str(i) + ' ended at ' + str(datetime.now()))\n",
    "        \n",
    "print('cv ended at ' + str(datetime.now()))\n",
    "\n",
    "np.set_printoptions(threshold=100000)\n",
    "curve_set=[None]*8\n",
    "\n",
    "curve_set[0]=tpr_list\n",
    "curve_set[1]=fpr_list\n",
    "curve_set[2]=thres_roc_list\n",
    "curve_set[3]=all_auc\n",
    "curve_set[4]=precision_list\n",
    "curve_set[5]=recall_list\n",
    "curve_set[6]=thres_pr_list\n",
    "curve_set[7]=all_aucpr\n",
    "\n",
    "fileexport_curve='Data/curve_mm.csv'\n",
    "pd.DataFrame(curve_set).to_csv(fileexport_curve,index=False, header=False)\n",
    "\n",
    "end=datetime.now()\n",
    "time.append(end-start)\n",
    "\n",
    "fileexport_time='Data/time_mm.csv'\n",
    "pd.DataFrame(time).to_csv(fileexport_time,index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
