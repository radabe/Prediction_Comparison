{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loguniform(low=0, high=1):\n",
    "    val = np.exp(np.random.uniform(0, 1, None))\n",
    "    scaled_val = (((val - np.exp(0)) * (high - low)) / (np.exp(1) - np.exp(0))) + low\n",
    "    return scaled_val\n",
    "\n",
    "def loguniform_int(low=0, high=1):\n",
    "    val = np.exp(np.random.uniform(0, 1, None))\n",
    "    scaled_val = (((val - np.exp(0)) * (high - low)) / (np.exp(1) - np.exp(0))) + low\n",
    "    return int(scaled_val)\n",
    "\n",
    "def uniform(low=0, high=1):\n",
    "    val = np.random.uniform(low, high, None)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(data, max_len):\n",
    "    data = data.sort_values('complete_timestamp', ascending=True, kind=\"mergesort\").groupby('unique_id').head(max_len)\n",
    "    grouped = data.sort_values('complete_timestamp', ascending=True, kind=\"mergesort\").groupby('unique_id')\n",
    "\n",
    "    data_dim = data.shape[1]-8\n",
    "    n_cases = data['unique_id'].nunique()\n",
    "\n",
    "    X = np.zeros((n_cases, max_len, data_dim), dtype=np.float32)\n",
    "    y = np.zeros((n_cases, 2), dtype=np.float32)\n",
    "\n",
    "    idx = 0\n",
    "    # each prefix will be a separate instance\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values('complete_timestamp', ascending=True, kind=\"mergesort\")\n",
    "        label = group['Releasetreue'].iloc[0]\n",
    "        group = group.to_numpy()\n",
    "        X[idx] = pad_sequences(group[np.newaxis,:30,8:], maxlen=max_len, dtype=np.float32)\n",
    "        y[idx, label] = 1\n",
    "        idx += 1\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCHistory(keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.aucs = []\n",
    "        self.aucs_pr_0 = []\n",
    "        self.aucs_pr_1 = []\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.aucs = []\n",
    "        self.aucs_pr = []\n",
    "        self.aucs_pr_0 = []\n",
    "        self.aucs_pr_1 = []\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.X_val)\n",
    "        self.aucs.append(roc_auc_score(self.y_val[:,0], y_pred[:,0]))\n",
    "        precision, recall, _ = precision_recall_curve(self.y_val[:,0],  y_pred[:,0])\n",
    "        self.aucs_pr_0.append(auc(recall, precision))\n",
    "        precision, recall, _ = precision_recall_curve(self.y_val[:,1],  y_pred[:,1])\n",
    "        self.aucs_pr_1.append(auc(recall, precision))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ea109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "best_auc=0\n",
    "best_param={}\n",
    "all_auc=[]\n",
    "all_param=[]\n",
    "best_aucpr=0\n",
    "all_aucpr=[]\n",
    "best_parampr={}\n",
    "time=[]\n",
    "confusion=[]\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "nb_epoch = 10\n",
    "start=datetime.now()\n",
    "max_len=30\n",
    "data_dim=train_lstm_ready.shape[1] - 7\n",
    "n_layers_values = [1, 2, 3]\n",
    "batch_size_values = [8, 16, 32, 64]\n",
    "optimizer_values = [\"rmsprop\", \"nadam\"]\n",
    "\n",
    "for i in range(16):\n",
    "     for k in range(5):\n",
    "        \n",
    "        print('run ' + str(i) + '.' + str(k) + ' started at ' + str(datetime.now()))\n",
    "        np.random.seed(i)\n",
    "        train_lstm_split = train_lstm_ready.reindex(np.random.permutation(train_lstm_ready.index))\n",
    "        val_ids = list(train_lstm_split['EC batch'].unique())[-int(val_ratio*len(train_lstm_split['EC batch'].unique())):]\n",
    "        val_df_pre = train_lstm_ready[train_lstm_ready['EC batch'].isin(val_ids)]\n",
    "        train_df_pre = train_lstm_ready[~train_lstm_ready['EC batch'].isin(val_ids)]\n",
    "\n",
    "        del train_lstm_split\n",
    "        \n",
    "        X_train, y_train = generate_3d_data(train_df_pre,max_len)      \n",
    "        X_val,y_val = generate_3d_data(val_df_pre,max_len)\n",
    "\n",
    "        del val_df_pre\n",
    "        del train_df_pre\n",
    "        \n",
    "        lstmsize = loguniform_int(10, 150)\n",
    "        dropout = uniform(0, 0.3)\n",
    "        n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "        batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "        optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "        learning_rate = loguniform(low=0.000001, high=0.0001)    \n",
    "\n",
    "        main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "        if n_layers == 1:\n",
    "            l2_3 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(main_input)\n",
    "            b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "        elif n_layers == 2:\n",
    "            l1 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(main_input)\n",
    "            b1 = BatchNormalization(axis=1)(l1)\n",
    "            l2_3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b1)\n",
    "            b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "        elif n_layers == 3:\n",
    "            l1 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(main_input)\n",
    "            b1 = BatchNormalization(axis=1)(l1)\n",
    "            l2 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(b1)\n",
    "            b2 = BatchNormalization(axis=1)(l2)\n",
    "            l3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b2)\n",
    "            b2_3 = BatchNormalization()(l3)\n",
    "\n",
    "        outcome_output = Dense(2, activation=activation, kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "\n",
    "        model = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "\n",
    "        if optimizer == \"nadam\":\n",
    "            opt = Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            opt = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "        model.compile(loss={'outcome_output':'binary_crossentropy'}, optimizer=opt, metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "        auc_cb = AUCHistory(X_val, y_val)\n",
    "        history = model.fit({'main_input': X_train}, {'outcome_output':y_train}, validation_data=(X_val, y_val),\n",
    "                            verbose=2, callbacks=[auc_cb], batch_size=batch_size, epochs=nb_epoch)\n",
    "\n",
    "        pr_auc=auc_cb.aucs_pr_1[-1]\n",
    "        lr_auc=auc_cb.aucs[-1]\n",
    "\n",
    "\n",
    "\n",
    "        if lr_auc>best_auc:\n",
    "            best_auc=lr_auc\n",
    "            best_param={'run ' +str(i)+'.'+str(k),'lstmsize: ' +str(lstmsize), lstmsize, 'dropout: ' +str(dropout), dropout, 'n_layers: ' +str(n_layers), n_layers,\n",
    "                        'batch_size: ' +str(batch_size), batch_size, 'optimizer:  '+str(optimizer), optimizer,'learning_rate:  '+str(learning_rate),learning_rate}\n",
    "\n",
    "        if pr_auc>best_aucpr:\n",
    "            best_aucpr=pr_auc\n",
    "            best_parampr={'run ' +str(i)+'.'+str(k),'lstmsize: ' +str(lstmsize), lstmsize, 'dropout: ' +str(dropout), dropout, 'n_layers: ' +str(n_layers), n_layers,\n",
    "                        'batch_size: ' +str(batch_size), batch_size, 'optimizer:  '+str(optimizer), optimizer,'learning_rate:  '+str(learning_rate),learning_rate}\n",
    "\n",
    "        all_auc.append(lr_auc)\n",
    "        all_aucpr.append(pr_auc)\n",
    "        all_param.append(['lstmsize= ' +str(lstmsize),'dropout= ' +str(dropout),'n_layers= ' +str(n_layers),'batch_size= ' +str(batch_size),\n",
    "                          'optimizer= ' +str(optimizer),'learning_rate= ' +str(learning_rate)])\n",
    "\n",
    "\n",
    "        print('run ' + str(i) + ' ended at ' + str(datetime.now()))\n",
    "        \n",
    "print('cv ended at ' + str(datetime.now()))\n",
    "\n",
    "end=datetime.now()\n",
    "time.append(end-start)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final testruns\n",
    "\n",
    "best_auc=0\n",
    "best_param={}\n",
    "all_auc=[]\n",
    "all_param=[]\n",
    "best_aucpr=0\n",
    "all_aucpr=[]\n",
    "best_parampr={}\n",
    "time=[]\n",
    "confusion=[]\n",
    "\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "thres_pr_list=[]\n",
    "fpr_list=[]\n",
    "tpr_list=[]\n",
    "thres_roc_list=[]\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "nb_epoch = 50\n",
    "start=datetime.now()\n",
    "max_len=30\n",
    "data_dim=train_lstm_ready.shape[1] - 7\n",
    "n_layers_values = [1, 2, 3]\n",
    "batch_size_values = [8, 16, 32, 64]\n",
    "optimizer_values = [\"rmsprop\", \"nadam\"]\n",
    "\n",
    "X_test, y_test = generate_3d_data(test_lstm_ready,max_len)\n",
    "\n",
    "for i in range(13,14):\n",
    "     for k in range(1):\n",
    "        \n",
    "        print('run ' + str(i) + '.' + str(k) + ' started at ' + str(datetime.now()))\n",
    "        np.random.seed(i)\n",
    "        train_lstm_split = train_lstm_ready.reindex(np.random.permutation(train_lstm_ready.index))\n",
    "        val_ids = list(train_lstm_split['EC batch'].unique())[-int(val_ratio*len(train_lstm_split['EC batch'].unique())):]\n",
    "        val_df_pre = train_lstm_ready[train_lstm_ready['EC batch'].isin(val_ids)]\n",
    "        train_df_pre = train_lstm_ready[~train_lstm_ready['EC batch'].isin(val_ids)]\n",
    "        \n",
    "        \n",
    "        val_split = val_df_pre.reindex(np.random.permutation(val_df_pre.index))\n",
    "        cal_ids = list(val_split['EC batch'].unique())[-int(cal_ratio*len(val_split['EC batch'].unique())):]\n",
    "        cal_df_pre = val_df_pre[val_df_pre['EC batch'].isin(cal_ids)]\n",
    "        val_df_pre = val_df_pre[~val_df_pre['EC batch'].isin(cal_ids)]\n",
    "\n",
    "        del train_lstm_split, val_split\n",
    "                      \n",
    "        X_train, y_train = generate_3d_data(train_df_pre,max_len)      \n",
    "        X_val,y_val = generate_3d_data(val_df_pre,max_len)\n",
    "        X_cal,y_cal = generate_3d_data(cal_df_pre,max_len)\n",
    "\n",
    "        del val_df_pre\n",
    "        del train_df_pre\n",
    "        del cal_df_pre\n",
    "        \n",
    "        lstmsize = loguniform_int(10, 150)\n",
    "        dropout = uniform(0, 0.3)\n",
    "        n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "        batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "        optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "        learning_rate = loguniform(low=0.000001, high=0.0001)    \n",
    "        \n",
    "        np.random.seed(i+k)\n",
    "\n",
    "        main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "        if n_layers == 1:\n",
    "            l2_3 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(main_input)\n",
    "            b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "        elif n_layers == 2:\n",
    "            l1 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(main_input)\n",
    "            b1 = BatchNormalization(axis=1)(l1)\n",
    "            l2_3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b1)\n",
    "            b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "        elif n_layers == 3:\n",
    "            l1 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(main_input)\n",
    "            b1 = BatchNormalization(axis=1)(l1)\n",
    "            l2 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(b1)\n",
    "            b2 = BatchNormalization(axis=1)(l2)\n",
    "            l3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b2)\n",
    "            b2_3 = BatchNormalization()(l3)\n",
    "\n",
    "        outcome_output = Dense(2, activation=activation, kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "\n",
    "        model = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "\n",
    "        if optimizer == \"nadam\":\n",
    "            opt = Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            opt = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "        model.compile(loss={'outcome_output':'binary_crossentropy'}, optimizer=opt, metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "        auc_cb = AUCHistory(X_val, y_val)\n",
    "        history = model.fit({'main_input': X_train}, {'outcome_output':y_train}, validation_data=(X_val, y_val),\n",
    "                            verbose=2, callbacks=[auc_cb], batch_size=batch_size, epochs=nb_epoch)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        # Data to plot precision - recall curve\n",
    "        precision, recall, thres_pr = precision_recall_curve(y_test[:,1],  y_pred[:,1])\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        thres_pr_list.append(thres_pr)\n",
    "        \n",
    "        # Data to plot roc curve\n",
    "        fpr, tpr, thres_roc = roc_curve(y_test[:,1],  y_pred[:,1])\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "        thres_roc_list.append(thres_roc)\n",
    "        \n",
    "        # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "        all_aucpr.append(auc(recall, precision))\n",
    "        \n",
    "        # calculate AUC ROC value\n",
    "        all_auc.append(roc_auc_score(y_test[:,1], y_pred[:,1]))\n",
    "\n",
    "\n",
    "        print('run ' + str(i) + ' ended at ' + str(datetime.now()))\n",
    "        \n",
    "print('cv ended at ' + str(datetime.now()))\n",
    "\n",
    "end=datetime.now()\n",
    "time.append(end-start)\n",
    "\n",
    "np.set_printoptions(threshold=100000)\n",
    "curve_set=[None]*8\n",
    "\n",
    "curve_set[0]=tpr_list\n",
    "curve_set[1]=fpr_list\n",
    "curve_set[2]=thres_roc_list\n",
    "curve_set[3]=all_auc\n",
    "curve_set[4]=precision_list\n",
    "curve_set[5]=recall_list\n",
    "curve_set[6]=thres_pr_list\n",
    "curve_set[7]=all_aucpr\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a94cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with buckets\n",
    "\n",
    "best_auc=0\n",
    "best_param={}\n",
    "all_auc=[]\n",
    "all_param=[]\n",
    "best_aucpr=0\n",
    "all_aucpr=[]\n",
    "best_parampr={}\n",
    "time=[]\n",
    "confusion=[]\n",
    "\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "thres_pr_list=[]\n",
    "fpr_list=[]\n",
    "tpr_list=[]\n",
    "thres_roc_list=[]\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "nb_epoch = 20\n",
    "start=datetime.now()\n",
    "max_len=30\n",
    "data_dim=train_lstm_ready.shape[1] - 7\n",
    "n_layers_values = [1, 2, 3]\n",
    "batch_size_values = [8, 16, 32, 64]\n",
    "optimizer_values = [\"rmsprop\", \"nadam\"]\n",
    "\n",
    "for n in range(2):\n",
    "    for m in range(2):\n",
    "\n",
    "        precision_list=[]\n",
    "        recall_list=[]\n",
    "        thres_pr_list=[]\n",
    "        fpr_list=[]\n",
    "        tpr_list=[]\n",
    "        thres_roc_list=[]\n",
    "        \n",
    "        train_df_bucket=train_lstm_ready[((train_lstm_ready['is_commercial']==n) &  (train_lstm_ready['is_quality']==m))]\n",
    "        test_df_bucket=test_lstm_ready[((test_lstm_ready['is_commercial']==n) &  (test_lstm_ready['is_quality']==m))]\n",
    "\n",
    "        X_test, y_test = generate_3d_data(test_df_bucket,max_len)\n",
    "\n",
    "        for i in range(13,14):\n",
    "             for k in range(20):\n",
    "\n",
    "                print('run ' + str(i) + '.' + str(k) + ' started at ' + str(datetime.now()))\n",
    "                np.random.seed(i)\n",
    "                train_lstm_split = train_df_bucket.reindex(np.random.permutation(train_df_bucket.index))\n",
    "                val_ids = list(train_lstm_split['EC batch'].unique())[-int(val_ratio*len(train_lstm_split['EC batch'].unique())):]\n",
    "                val_df_pre = train_df_bucket[train_df_bucket['EC batch'].isin(val_ids)]\n",
    "                train_df_pre = train_df_bucket[~train_df_bucket['EC batch'].isin(val_ids)]\n",
    "\n",
    "                del train_lstm_split\n",
    "\n",
    "                X_train, y_train = generate_3d_data(train_df_pre,max_len)      \n",
    "                X_val,y_val = generate_3d_data(val_df_pre,max_len)\n",
    "\n",
    "                del val_df_pre\n",
    "                del train_df_pre\n",
    "\n",
    "                lstmsize = loguniform_int(10, 150)\n",
    "                dropout = uniform(0, 0.3)\n",
    "                n_layers = n_layers_values[np.random.randint(0, len(n_layers_values))]\n",
    "                batch_size = batch_size_values[np.random.randint(0, len(batch_size_values))]\n",
    "                optimizer = optimizer_values[np.random.randint(0, len(optimizer_values))]\n",
    "                learning_rate = loguniform(low=0.000001, high=0.0001)    \n",
    "\n",
    "                np.random.seed(i+k)\n",
    "\n",
    "                main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                if n_layers == 1:\n",
    "                    l2_3 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(main_input)\n",
    "                    b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                elif n_layers == 2:\n",
    "                    l1 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(main_input)\n",
    "                    b1 = BatchNormalization(axis=1)(l1)\n",
    "                    l2_3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b1)\n",
    "                    b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                elif n_layers == 3:\n",
    "                    l1 = LSTM(lstmsize, input_shape=(max_len, data_dim), implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(main_input)\n",
    "                    b1 = BatchNormalization(axis=1)(l1)\n",
    "                    l2 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=dropout)(b1)\n",
    "                    b2 = BatchNormalization(axis=1)(l2)\n",
    "                    l3 = LSTM(lstmsize, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=dropout)(b2)\n",
    "                    b2_3 = BatchNormalization()(l3)\n",
    "\n",
    "                outcome_output = Dense(2, activation=activation, kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "\n",
    "                model = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "\n",
    "                if optimizer == \"nadam\":\n",
    "                    opt = Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                elif optimizer == \"rmsprop\":\n",
    "                    opt = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "                model.compile(loss={'outcome_output':'binary_crossentropy'}, optimizer=opt, metrics=[tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "                auc_cb = AUCHistory(X_val, y_val)\n",
    "                history = model.fit({'main_input': X_train}, {'outcome_output':y_train}, validation_data=(X_val, y_val),\n",
    "                                    verbose=2, callbacks=[auc_cb], batch_size=batch_size, epochs=nb_epoch)\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "                # Data to plot precision - recall curve\n",
    "                precision, recall, thres_pr = precision_recall_curve(y_test[:,1],  y_pred[:,1])\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "                thres_pr_list.append(thres_pr)\n",
    "\n",
    "                # Data to plot roc curve\n",
    "                fpr, tpr, thres_roc = roc_curve(y_test[:,1],  y_pred[:,1])\n",
    "                fpr_list.append(fpr)\n",
    "                tpr_list.append(tpr)\n",
    "                thres_roc_list.append(thres_roc)\n",
    "\n",
    "                # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "                all_aucpr.append(auc(recall, precision))\n",
    "\n",
    "                # calculate AUC ROC value\n",
    "                all_auc.append(roc_auc_score(y_test[:,1], y_pred[:,1]))\n",
    "\n",
    "\n",
    "                print('run ' + str(i) + ' ended at ' + str(datetime.now()))\n",
    "\n",
    "        print('cv ended at ' + str(datetime.now()))\n",
    "\n",
    "        end=datetime.now()\n",
    "        time.append(end-start)\n",
    "\n",
    "        np.set_printoptions(threshold=100000)\n",
    "        curve_set=[None]*8\n",
    "\n",
    "        curve_set[0]=tpr_list\n",
    "        curve_set[1]=fpr_list\n",
    "        curve_set[2]=thres_roc_list\n",
    "        curve_set[3]=all_auc\n",
    "        curve_set[4]=precision_list\n",
    "        curve_set[5]=recall_list\n",
    "        curve_set[6]=thres_pr_list\n",
    "        curve_set[7]=all_aucpr\n",
    "\n",
    "        fileexport_curve='Data/curve_lstm_' + str(n) + '_' + str(m) + '.csv'\n",
    "        pd.DataFrame(curve_set).to_csv(fileexport_curve,index=False, header=False)\n",
    "\n",
    "        end=datetime.now()\n",
    "        time.append(end-start)\n",
    "\n",
    "        fileexport_time='Data/time_lstm_' + str(n) + '_' + str(m) + '.csv'\n",
    "        pd.DataFrame(time).to_csv(fileexport_time,index=False, header=False)\n",
    "        \n",
    "        del train_df_bucket, test_df_bucket\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
